{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf499b8-4aa4-4ecd-995c-cc26ca072020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments, AutoModelForSequenceClassification, EarlyStoppingCallback, TrainerCallback\n",
    "from pathlib import Path\n",
    "import torch, random\n",
    "import librosa, os\n",
    "import IPython.display as ipd\n",
    "from dotenv import load_dotenv\n",
    "import evaluate\n",
    "from spellchecker import SpellChecker\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import DefaultDataCollator\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, Trainer, TrainingArguments\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381ec537-eeec-410c-900e-373134150568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>transcript</th>\n",
       "      <th>tool</th>\n",
       "      <th>heading</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Turret, prepare to deploy electromagnetic puls...</td>\n",
       "      <td>electromagnetic pulse</td>\n",
       "      <td>065</td>\n",
       "      <td>grey and white fighter jet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Engage yellow drone with surface-to-air missil...</td>\n",
       "      <td>surface-to-air missiles</td>\n",
       "      <td>235</td>\n",
       "      <td>yellow drone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Control to turrets, deploy electromagnetic pul...</td>\n",
       "      <td>electromagnetic pulse</td>\n",
       "      <td>110</td>\n",
       "      <td>blue and red fighter plane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Alfa, Echo, Mike Papa, deploy EMP tool heading...</td>\n",
       "      <td>EMP</td>\n",
       "      <td>085</td>\n",
       "      <td>purple, red, and silver fighter jet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Engage the grey, black, and green fighter plan...</td>\n",
       "      <td>machine gun</td>\n",
       "      <td>095</td>\n",
       "      <td>grey, black, and green fighter plane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>3495</td>\n",
       "      <td>Deploy electromagnetic pulse on brown commerci...</td>\n",
       "      <td>electromagnetic pulse</td>\n",
       "      <td>350</td>\n",
       "      <td>brown commercial aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>3496</td>\n",
       "      <td>Deploy surface-to-air missiles, heading two on...</td>\n",
       "      <td>surface-to-air missiles</td>\n",
       "      <td>215</td>\n",
       "      <td>silver, orange, and brown helicopter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>3497</td>\n",
       "      <td>Engage target, grey, orange, and silver missil...</td>\n",
       "      <td>surface-to-air missiles</td>\n",
       "      <td>080</td>\n",
       "      <td>grey, orange, and silver missile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>3498</td>\n",
       "      <td>Engage the white drone at heading zero five fi...</td>\n",
       "      <td>machine gun</td>\n",
       "      <td>055</td>\n",
       "      <td>white drone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>3499</td>\n",
       "      <td>Turret Charlie, prepare to engage. Deploy EMP ...</td>\n",
       "      <td>EMP</td>\n",
       "      <td>255</td>\n",
       "      <td>white cargo aircraft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       key                                         transcript  \\\n",
       "0        0  Turret, prepare to deploy electromagnetic puls...   \n",
       "1        1  Engage yellow drone with surface-to-air missil...   \n",
       "2        2  Control to turrets, deploy electromagnetic pul...   \n",
       "3        3  Alfa, Echo, Mike Papa, deploy EMP tool heading...   \n",
       "4        4  Engage the grey, black, and green fighter plan...   \n",
       "...    ...                                                ...   \n",
       "3495  3495  Deploy electromagnetic pulse on brown commerci...   \n",
       "3496  3496  Deploy surface-to-air missiles, heading two on...   \n",
       "3497  3497  Engage target, grey, orange, and silver missil...   \n",
       "3498  3498  Engage the white drone at heading zero five fi...   \n",
       "3499  3499  Turret Charlie, prepare to engage. Deploy EMP ...   \n",
       "\n",
       "                         tool heading                                target  \n",
       "0       electromagnetic pulse     065            grey and white fighter jet  \n",
       "1     surface-to-air missiles     235                          yellow drone  \n",
       "2       electromagnetic pulse     110            blue and red fighter plane  \n",
       "3                         EMP     085   purple, red, and silver fighter jet  \n",
       "4                 machine gun     095  grey, black, and green fighter plane  \n",
       "...                       ...     ...                                   ...  \n",
       "3495    electromagnetic pulse     350             brown commercial aircraft  \n",
       "3496  surface-to-air missiles     215  silver, orange, and brown helicopter  \n",
       "3497  surface-to-air missiles     080      grey, orange, and silver missile  \n",
       "3498              machine gun     055                           white drone  \n",
       "3499                      EMP     255                  white cargo aircraft  \n",
       "\n",
       "[3500 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "TEAM_NAME = os.getenv(\"TEAM_NAME\", \"7up\")\n",
    "TEAM_TRACK = os.getenv(\"TEAM_TRACK\", \"advanced\")\n",
    "\n",
    "\n",
    "input_dir = Path(f\"/home/jupyter/{TEAM_TRACK}\")\n",
    "# input_dir = Path(f\"../../data/{TEAM_TRACK}/train\")\n",
    "results_dir = Path(f\"/home/jupyter/{TEAM_NAME}\")\n",
    "# results_dir = Path(\"results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "data = []\n",
    "with jsonlines.open(input_dir / \"nlp.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0c6120-0ef8-410f-a801-f9c2ed0231f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>turret, prepare to deploy electromagnetic puls...</td>\n",
       "      <td>What is the target?</td>\n",
       "      <td>grey and white fighter jet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>turret, prepare to deploy electromagnetic puls...</td>\n",
       "      <td>What is the heading?</td>\n",
       "      <td>zero six five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>turret, prepare to deploy electromagnetic puls...</td>\n",
       "      <td>What is the tool to be deployed?</td>\n",
       "      <td>electromagnetic pulse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>engage yellow drone with surface-to-air missil...</td>\n",
       "      <td>What is the target?</td>\n",
       "      <td>yellow drone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>engage yellow drone with surface-to-air missil...</td>\n",
       "      <td>What is the heading?</td>\n",
       "      <td>two three five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>engage the white drone at heading zero five fi...</td>\n",
       "      <td>What is the heading?</td>\n",
       "      <td>zero five five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>engage the white drone at heading zero five fi...</td>\n",
       "      <td>What is the tool to be deployed?</td>\n",
       "      <td>machine gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>turret charlie, prepare to engage. deploy emp ...</td>\n",
       "      <td>What is the target?</td>\n",
       "      <td>white cargo aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>turret charlie, prepare to engage. deploy emp ...</td>\n",
       "      <td>What is the heading?</td>\n",
       "      <td>two five five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>turret charlie, prepare to engage. deploy emp ...</td>\n",
       "      <td>What is the tool to be deployed?</td>\n",
       "      <td>EMP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10500 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "0      turret, prepare to deploy electromagnetic puls...   \n",
       "1      turret, prepare to deploy electromagnetic puls...   \n",
       "2      turret, prepare to deploy electromagnetic puls...   \n",
       "3      engage yellow drone with surface-to-air missil...   \n",
       "4      engage yellow drone with surface-to-air missil...   \n",
       "...                                                  ...   \n",
       "10495  engage the white drone at heading zero five fi...   \n",
       "10496  engage the white drone at heading zero five fi...   \n",
       "10497  turret charlie, prepare to engage. deploy emp ...   \n",
       "10498  turret charlie, prepare to engage. deploy emp ...   \n",
       "10499  turret charlie, prepare to engage. deploy emp ...   \n",
       "\n",
       "                               question                      answer  \n",
       "0                   What is the target?  grey and white fighter jet  \n",
       "1                  What is the heading?               zero six five  \n",
       "2      What is the tool to be deployed?       electromagnetic pulse  \n",
       "3                   What is the target?                yellow drone  \n",
       "4                  What is the heading?              two three five  \n",
       "...                                 ...                         ...  \n",
       "10495              What is the heading?              zero five five  \n",
       "10496  What is the tool to be deployed?                 machine gun  \n",
       "10497               What is the target?        white cargo aircraft  \n",
       "10498              What is the heading?               two five five  \n",
       "10499  What is the tool to be deployed?                         EMP  \n",
       "\n",
       "[10500 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_contexts = []\n",
    "new_questions = []\n",
    "new_answers = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    context = row['transcript']\n",
    "    for col in ['target', 'heading', 'tool']:\n",
    "        new_contexts.append(context)\n",
    "        if col == 'tool':\n",
    "            new_questions.append(f\"What is the tool to be deployed?\")\n",
    "        else:\n",
    "            new_questions.append(f\"What is the {col}?\")\n",
    "        new_answers.append(row[col])\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "    'context': new_contexts,\n",
    "    'question': new_questions,\n",
    "    'answer': new_answers\n",
    "})\n",
    "\n",
    "def convert_number_to_words(answer):\n",
    "    if answer.isdigit():  # Check if the answer is a digit\n",
    "        return ' '.join(p.number_to_words(digit) for digit in answer)\n",
    "    return answer\n",
    "\n",
    "# Apply the function to the answer column\n",
    "new_df['answer'] = new_df['answer'].apply(convert_number_to_words)\n",
    "new_df['context'] = new_df['context'].str.lower()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0cfdb7-4919-4dd9-be28-609869b3297d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416e2df55e9048e5bee5291adea3065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf001e0384674d7fb9bec2987797ddc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'question', 'answer', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'],\n",
       "    num_rows: 8400\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(new_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        stride=128\n",
    "    )\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offsets in enumerate(inputs['offset_mapping']):\n",
    "        try:\n",
    "            start_char = examples['context'][i].index(examples['answer'][i].lower())\n",
    "            end_char = start_char + len(examples['answer'][i])\n",
    "        except ValueError:\n",
    "            # Handle cases where answer is not found in context\n",
    "            try:\n",
    "                start_char = examples['context'][i].index(examples['answer'][i].lower().replace(\"nine\", \"niner\"))\n",
    "            except ValueError:\n",
    "                print(examples['context'][i], examples['answer'][i])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        \n",
    "        token_end_index = len(inputs['input_ids'][i]) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        while offsets[token_start_index][0] <= start_char:\n",
    "            token_start_index += 1\n",
    "        while offsets[token_end_index][1] >= end_char:\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        start_positions.append(token_start_index - 1)\n",
    "        end_positions.append(token_end_index + 1)\n",
    "    \n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions\n",
    "    return inputs\n",
    "\n",
    "# Preprocess the datasets\n",
    "train_tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_tokenized_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "train_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa00ecd7-3b5e-44b7-bd5e-bf8edd6233c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2200' max='2200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2200/2200 20:22, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.296782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.087126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.050711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.041639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.040205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.040079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.040241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.039052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.038305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.038692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.038469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2200, training_loss=0.246962609724565, metrics={'train_runtime': 1227.834, 'train_samples_per_second': 7.167, 'train_steps_per_second': 1.792, 'total_flos': 1724558594457600.0, 'train_loss': 0.246962609724565, 'epoch': 1.0476190476190477})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distil-bert-ft-qa-model-7up-v7\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=7e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    max_steps=2200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfa7b7-978c-4103-a3fc-452b260da85b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Prepare inputs for tokenization\n",
    "# questions = new_df[\"question\"].tolist()\n",
    "# contexts = new_df[\"context\"].tolist()\n",
    "# answers = new_df[\"answer\"].tolist()\n",
    "\n",
    "# # Tokenize the inputs\n",
    "# inputs = tokenizer(\n",
    "#     questions,\n",
    "#     contexts,\n",
    "#     max_length=384,\n",
    "#     truncation=\"only_second\",\n",
    "#     return_offsets_mapping=True,\n",
    "#     padding=\"max_length\",\n",
    "# )\n",
    "\n",
    "# offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "# start_positions = []\n",
    "# end_positions = []\n",
    "\n",
    "# for i, offset in enumerate(offset_mapping):\n",
    "#     answer = answers[i]\n",
    "#     start_char = contexts[i].find(answer)\n",
    "#     end_char = start_char + len(answer)\n",
    "    \n",
    "#     sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "#     # Find the start and end of the context\n",
    "#     context_start = 0\n",
    "#     while sequence_ids[context_start] != 1:\n",
    "#         context_start += 1\n",
    "#     context_end = context_start\n",
    "#     while context_end < len(sequence_ids) and sequence_ids[context_end] == 1:\n",
    "#         context_end += 1\n",
    "#     context_end -= 1\n",
    "\n",
    "#     # If the answer is not fully inside the context, label it (0, 0)\n",
    "#     if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "#         start_positions.append(0)\n",
    "#         end_positions.append(0)\n",
    "#     else:\n",
    "#         # Find start token position\n",
    "#         idx = context_start\n",
    "#         while idx <= context_end and offset[idx][0] <= start_char:\n",
    "#             idx += 1\n",
    "#         start_positions.append(idx - 1)\n",
    "\n",
    "#         # Find end token position\n",
    "#         idx = context_end\n",
    "#         while idx >= context_start and offset[idx][1] >= end_char:\n",
    "#             idx -= 1\n",
    "#         end_positions.append(idx + 1)\n",
    "\n",
    "# new_df[\"start_positions\"] = start_positions\n",
    "# new_df[\"end_positions\"] = end_positions\n",
    "\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd199a-5d9f-4c73-a52d-879f1e637fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "\n",
    "# train_df, test_df = train_test_split(new_df, test_size=0.1, random_state=seed, shuffle=True)\n",
    "\n",
    "# # Display the resulting DataFrames\n",
    "# print(\"Training Set:\")\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6e9ea-fcaf-452f-a906-6fed29f67b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"context\"], examples[\"question\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Ensure that the tokenized fields are included in the dataset\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"context\", \"question\", \"answer\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"context\", \"question\", \"answer\"])\n",
    "\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef88dd8-5608-463b-950e-95bc164a18b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 18:44, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.031200</td>\n",
       "      <td>1.820730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.534400</td>\n",
       "      <td>1.353820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.312100</td>\n",
       "      <td>1.061962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.020200</td>\n",
       "      <td>0.868066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.814651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.716137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.769623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.657700</td>\n",
       "      <td>0.579359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>0.526296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.493492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=1.078759828186035, metrics={'train_runtime': 1126.0222, 'train_samples_per_second': 8.881, 'train_steps_per_second': 8.881, 'total_flos': 1306531000320000.0, 'train_loss': 1.078759828186035, 'epoch': 1.0582010582010581})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./distil-bert-ft-qa-model-7up\",\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     max_steps=10000,\n",
    "#     eval_steps=1000,\n",
    "#     weight_decay=0.01,\n",
    "#     push_to_hub=True,\n",
    "# )\n",
    "\n",
    "# # Data collator\n",
    "# data_collator = DefaultDataCollator()\n",
    "\n",
    "# # Trainer setup\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06661f05-935d-4110-b32e-07c6aaaf5835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heading': 'zero six five', 'tool': 'electromagnetic pulse', 'target': 'zero six five. target is an incoming missile'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "class NLPManager:\n",
    "    def __init__(self):\n",
    "        # Initialize the model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained('cadzchua/distil-bert-ft-qa-model-7up-v6')  \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def qa(self, context: str) -> Dict[str, str]:\n",
    "        # Perform NLP question-answering\n",
    "        context = context.lower()\n",
    "        hdg_ans = self.find_heading(context)\n",
    "        # hdg_ans = self.heading_text_to_string(hdg_ans)\n",
    "        tool_ans = self.predict_answer(\"What is the tool to be deployed?\", context)\n",
    "        if \"-\" in tool_ans:\n",
    "            tool_ans = self.remove_spaces_around_hyphens(tool_ans)\n",
    "        if tool_ans.lower() in ['emp', 'emp tool']:\n",
    "            tool_ans = 'EMP'\n",
    "        tgt_ans = self.predict_answer(\"What is the target?\", context)\n",
    "        \n",
    "        return {\"heading\": hdg_ans, \"tool\": tool_ans, \"target\": tgt_ans}\n",
    "    \n",
    "    def find_heading(self, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the heading from the context using POS tagging.\n",
    "        \"\"\"\n",
    "        words = word_tokenize(context)\n",
    "        pos_tags = pos_tag(words)\n",
    "        \n",
    "        finding_heading = True\n",
    "        reading_heading = False\n",
    "        heading = []\n",
    "\n",
    "        for word, pos in pos_tags:\n",
    "            if finding_heading:\n",
    "                if reading_heading:\n",
    "                    if pos in ['CD', 'NN', 'JJ']:\n",
    "                        heading.append(word)\n",
    "                    # else:\n",
    "                    #     reading_heading = False\n",
    "                    #     finding_heading = False\n",
    "                \n",
    "                if word.lower() in ['heading', 'at']:\n",
    "                    reading_heading = True\n",
    "\n",
    "        return \" \".join(heading)\n",
    "    \n",
    "    def heading_text_to_string(self, heading_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Convert heading text to string representation with leading zeros.\n",
    "        \"\"\"\n",
    "        heading_mapping = {\n",
    "            \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n",
    "            \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\", \"niner\": \"9\"\n",
    "        }\n",
    "        words = heading_text.split()\n",
    "        string_heading = \"\".join([heading_mapping.get(word, \"\") for word in words])\n",
    "        return string_heading.zfill(3)  # Ensure the heading is always three digits\n",
    "    \n",
    "    def predict_answer(self, question: str, context: str) -> str:\n",
    "        inputs = self.tokenizer.encode_plus(question, context, return_tensors='pt', truncation=True, padding=\"max_length\").to(self.device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "        answer = self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids[0][start_index:end_index]))\n",
    "        return answer.strip()\n",
    "    \n",
    "    def remove_spaces_around_hyphens(self, text: str) -> str:\n",
    "        return re.sub(r'\\s*-\\s*', '-', text)\n",
    "    \n",
    "nlp_manager = NLPManager()\n",
    "\n",
    "# Test with a new context\n",
    "context = \"\"\"\n",
    "Control tower to air defense turrets, deploy electromagnetic pulse at heading zero six five. \n",
    "Target is an incoming missile.\n",
    "\"\"\"\n",
    "result = nlp_manager.qa(context)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f5b8d7-1dce-4a5b-a713-46dfa7e74eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator\n",
    "# from datasets import Dataset\n",
    "\n",
    "# # Example DataFrame\n",
    "# data = {\n",
    "#     'context': ['context1', 'context2'],\n",
    "#     'tool': ['tool1', 'tool2'],\n",
    "#     'heading': ['heading1', 'heading2'],\n",
    "#     'target': ['target1', 'target2']\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Initialize lists to store the new DataFrame's data\n",
    "# new_contexts = []\n",
    "# new_questions = []\n",
    "# new_answers = []\n",
    "\n",
    "# # Loop through each row in the original DataFrame\n",
    "# for index, row in df.iterrows():\n",
    "#     context = row['context']\n",
    "#     for col in ['target', 'heading', 'tool']:\n",
    "#         new_contexts.append(context)\n",
    "#         if col == 'tool':\n",
    "#             new_questions.append(\"What is the tool to be deployed?\")\n",
    "#         else:\n",
    "#             new_questions.append(f\"What is the {col}?\")\n",
    "#         new_answers.append(row[col])\n",
    "\n",
    "# # Create the new DataFrame\n",
    "# new_df = pd.DataFrame({\n",
    "#     'context': new_contexts,\n",
    "#     'question': new_questions,\n",
    "#     'answer': new_answers\n",
    "# })\n",
    "\n",
    "# # Tokenizer setup\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Prepare inputs for tokenization\n",
    "# questions = new_df[\"question\"].tolist()\n",
    "# contexts = new_df[\"context\"].tolist()\n",
    "# answers = new_df[\"answer\"].tolist()\n",
    "\n",
    "# # Tokenize the inputs\n",
    "# inputs = tokenizer(\n",
    "#     questions,\n",
    "#     contexts,\n",
    "#     max_length=384,\n",
    "#     truncation=\"only_second\",\n",
    "#     return_offsets_mapping=True,\n",
    "#     padding=\"max_length\",\n",
    "# )\n",
    "\n",
    "# offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "# start_positions = []\n",
    "# end_positions = []\n",
    "\n",
    "# for i, offset in enumerate(offset_mapping):\n",
    "#     answer = answers[i]\n",
    "#     start_char = contexts[i].find(answer)\n",
    "#     end_char = start_char + len(answer)\n",
    "    \n",
    "#     sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "#     # Find the start and end of the context\n",
    "#     context_start = 0\n",
    "#     while sequence_ids[context_start] != 1:\n",
    "#         context_start += 1\n",
    "#     context_end = context_start\n",
    "#     while context_end < len(sequence_ids) and sequence_ids[context_end] == 1:\n",
    "#         context_end += 1\n",
    "#     context_end -= 1\n",
    "\n",
    "#     # If the answer is not fully inside the context, label it (0, 0)\n",
    "#     if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "#         start_positions.append(0)\n",
    "#         end_positions.append(0)\n",
    "#     else:\n",
    "#         # Find start token position\n",
    "#         idx = context_start\n",
    "#         while idx <= context_end and offset[idx][0] <= start_char:\n",
    "#             idx += 1\n",
    "#         start_positions.append(idx - 1)\n",
    "\n",
    "#         # Find end token position\n",
    "#         idx = context_end\n",
    "#         while idx >= context_start and offset[idx][1] >= end_char:\n",
    "#             idx -= 1\n",
    "#         end_positions.append(idx + 1)\n",
    "\n",
    "# new_df[\"start_positions\"] = start_positions\n",
    "# new_df[\"end_positions\"] = end_positions\n",
    "\n",
    "# # Split the data\n",
    "# seed = 42\n",
    "# train_df, test_df = train_test_split(new_df, test_size=0.1, random_state=seed, shuffle=True)\n",
    "\n",
    "# # Convert to Hugging Face Dataset\n",
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "# test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# # Load model\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./bert-qa-model-7up\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     push_to_hub=True,\n",
    "# )\n",
    "\n",
    "# # Data collator\n",
    "# data_collator = DefaultDataCollator()\n",
    "\n",
    "# # Trainer setup\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc3d412e-201d-415a-b7c8-d553fa4f743d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Control', 'to', 'air', 'defense', 'turrets', ',', 'we', 'have', 'a', 'target', 'consisting', 'of', 'a', 'red', ',', 'brown', ',', 'and', 'orange', 'drone', 'heading', 'towards', 'your', 'location', '.', 'Please', 'deploy', 'the', 'drone', 'catcher', 'and', 'intercept', 'the', 'target', 'at', 'heading', 'zero', 'three', 'zero', '.', 'Over', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"Control to air defense turrets, we have a target consisting of a red, brown, and orange drone heading towards your location. Please deploy the drone catcher and intercept the target at heading zero three zero. Over.\"\n",
    "tokenized_text = word_tokenize(sentence)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6664705e-eb70-4312-867b-43a54ee92a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Control', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('air', 'NN'),\n",
       " ('defense', 'NN'),\n",
       " ('turrets', 'NNS'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('target', 'NN'),\n",
       " ('consisting', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('red', 'JJ'),\n",
       " (',', ','),\n",
       " ('brown', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('orange', 'NN'),\n",
       " ('drone', 'NN'),\n",
       " ('heading', 'VBG'),\n",
       " ('towards', 'NNS'),\n",
       " ('your', 'PRP$'),\n",
       " ('location', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Please', 'NNP'),\n",
       " ('deploy', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('drone', 'NN'),\n",
       " ('catcher', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('intercept', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('target', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('heading', 'VBG'),\n",
       " ('zero', 'CD'),\n",
       " ('three', 'CD'),\n",
       " ('zero', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Over', 'IN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = tokens_tag = pos_tag(tokenized_text)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd81d7b-32e7-4b04-a4d8-fb0e555b03ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def extract_information(word_pos_pairs):\n",
    "#     tool = None\n",
    "#     target = []\n",
    "#     heading = []\n",
    "\n",
    "#     reading_tool = False\n",
    "#     reading_target = False\n",
    "#     reading_heading = False\n",
    "#     finding_tool = True\n",
    "#     finding_heading = True\n",
    "#     finding_target = True\n",
    "\n",
    "#     for word, pos in word_pos_pairs:\n",
    "#         # Identify the tool\n",
    "#         if finding_tool:\n",
    "#             if reading_tool:\n",
    "#                 if pos in ['JJ', 'NN', 'NNS', 'NNP']:\n",
    "#                     if word.lower() not in ['towards', 'deployment']:\n",
    "#                         tool = (tool + \" \" + word) if tool else word\n",
    "#                     else:\n",
    "#                         finding_tool = False\n",
    "#                 elif pos in ['DT', 'IN']:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     reading_tool = False\n",
    "#                     finding_tool = False\n",
    "        \n",
    "#             if word.lower() in ['deploy', 'with', 'using', 'deployment', 'initiate', 'initiating']:\n",
    "#                 reading_tool = True\n",
    "\n",
    "#         if finding_heading:\n",
    "#             if reading_heading:\n",
    "#                 if pos in ['CD', 'NN', 'JJ']:\n",
    "#                     heading.append(word)\n",
    "#                 else:\n",
    "#                     reading_heading = False\n",
    "#                     finding_heading = False\n",
    "        \n",
    "#             if word.lower() in ['heading', 'at']:\n",
    "#                 reading_heading = True\n",
    "        \n",
    "#         if finding_target:\n",
    "#             if reading_target:\n",
    "#                 if pos in ['JJ', 'CC', 'NN', 'RB', ',']:  # Include more POS tags to capture full target phrase\n",
    "#                     target.append(word)\n",
    "#                 elif word == \".\":\n",
    "#                     break\n",
    "#                 elif pos in ['VBZ', 'DT']:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     reading_target = False\n",
    "#                     finding_target = False\n",
    "                    \n",
    "#             if (word.lower()in [\"target\", \"engage\", \"towards\"]):\n",
    "#                 reading_target = True\n",
    "\n",
    "#     # Join the heading and target words into strings\n",
    "#     heading_str = ' '.join(heading)\n",
    "#     target_str = ' '.join(target)\n",
    "\n",
    "#     return tool, heading_str, target_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "696d8464-d90c-4480-9336-0f5dc36d263e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insane missle'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_ans = \"attack helicopter using insane missle\"\n",
    "ans = tool_ans.split('using', 1)[1].strip()\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "41886c47-3fea-4084-a698-f93c8badf227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 'one eight zero', 'bogey')\n"
     ]
    }
   ],
   "source": [
    "# hi = extract_information(tags)\n",
    "# print(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce68f2a-6565-4f85-8140-81c1632382ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
