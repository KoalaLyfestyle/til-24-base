{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8b727d-d408-4535-92b8-c057da8c12df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import torch, random\n",
    "import librosa, os\n",
    "import IPython.display as ipd\n",
    "from dotenv import load_dotenv\n",
    "from spellchecker import SpellChecker\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import ops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6007e56-bd09-4eaf-8f57-1cf44c71b022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_0.jpg</td>\n",
       "      <td>[grey missile, red, white, and blue light airc...</td>\n",
       "      <td>[[912, 164, 48, 152], [1032, 80, 24, 28], [704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_1.jpg</td>\n",
       "      <td>[grey camouflage fighter jet, grey and white f...</td>\n",
       "      <td>[[1112, 172, 64, 36], [1108, 512, 144, 48], [3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_10.jpg</td>\n",
       "      <td>[blue and grey fighter jet, blue helicopter, b...</td>\n",
       "      <td>[[792, 208, 52, 40], [964, 392, 52, 52], [408,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_100.jpg</td>\n",
       "      <td>[grey camouflage fighter jet, white, black, an...</td>\n",
       "      <td>[[1108, 224, 56, 44], [788, 148, 92, 32], [516...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_1000.jpg</td>\n",
       "      <td>[red and white fighter plane, yellow commercia...</td>\n",
       "      <td>[[412, 252, 68, 36], [544, 276, 44, 36], [1092...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image                                            caption  \\\n",
       "0     image_0.jpg  [grey missile, red, white, and blue light airc...   \n",
       "1     image_1.jpg  [grey camouflage fighter jet, grey and white f...   \n",
       "2    image_10.jpg  [blue and grey fighter jet, blue helicopter, b...   \n",
       "3   image_100.jpg  [grey camouflage fighter jet, white, black, an...   \n",
       "4  image_1000.jpg  [red and white fighter plane, yellow commercia...   \n",
       "\n",
       "                                                bbox  \n",
       "0  [[912, 164, 48, 152], [1032, 80, 24, 28], [704...  \n",
       "1  [[1112, 172, 64, 36], [1108, 512, 144, 48], [3...  \n",
       "2  [[792, 208, 52, 40], [964, 392, 52, 52], [408,...  \n",
       "3  [[1108, 224, 56, 44], [788, 148, 92, 32], [516...  \n",
       "4  [[412, 252, 68, 36], [544, 276, 44, 36], [1092...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "data = {'image': [], 'caption': [], 'bbox': []}\n",
    "load_dotenv()\n",
    "\n",
    "TEAM_NAME = os.getenv(\"TEAM_NAME\", \"7up\")\n",
    "TEAM_TRACK = os.getenv(\"TEAM_TRACK\", \"advanced\")\n",
    "\n",
    "\n",
    "input_dir = Path(f\"/home/jupyter/{TEAM_TRACK}\")\n",
    "with jsonlines.open(input_dir / \"vlm.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        image = obj['image']\n",
    "        annotations = obj['annotations']\n",
    "        for annotation in annotations:\n",
    "            caption = annotation['caption']\n",
    "            bbox = annotation['bbox']\n",
    "            data['image'].append(image)\n",
    "            data['caption'].append(caption)\n",
    "            data['bbox'].append(bbox)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "grouped_df = df.groupby('image').agg(list).reset_index()\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e007f7-4801-4d10-b3c9-74c3bd45f9b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values = df['caption'].unique()\n",
    "len(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2231170a-a373-4720-b555-44f78f3d595d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3d7333d-f94b-4852-a03d-c1241abc2cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    '''\n",
    "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    images: torch.Tensor of size (B, C, H, W)\n",
    "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
    "    gt classes: torch.Tensor of size (B, max_objects)\n",
    "    '''\n",
    "    def __init__(self, df, img_dir, img_size, name2idx, transforms=None):\n",
    "        self.image = df[\"image\"]\n",
    "        self.caption = df[\"caption\"]\n",
    "        self.bbox = df[\"bbox\"]\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "        self.name2idx = name2idx\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "        \n",
    "        gt_bboxes = torch.tensor(self.bbox[idx], dtype=torch.float32)\n",
    "        gt_classes = torch.tensor([self.name2idx[name] for name in self.caption[idx]], dtype=torch.long)\n",
    "        \n",
    "        return img, gt_bboxes, gt_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89d05fba-7771-4f0e-ab38-4b55443d624a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_width = 1520\n",
    "img_height = 870\n",
    "df_input = df\n",
    "image_dir = \"/home/jupyter/advanced/images/\"\n",
    "name2idx = {value: idx for idx, value in enumerate(unique_values)}\n",
    "name2idx['pad'] = -1\n",
    "idx2name = {v:k for k, v in name2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf970c7c-e922-4174-97db-4bb9db115f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "od_dataset = ObjectDetectionDataset(grouped_df, image_dir, (img_height, img_width), name2idx, data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e885d8c-32ce-4f9d-bd07-61018604fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_dataloader = DataLoader(od_dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9df41f1-496e-4f32-83c6-32d82111666b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4, 4] at entry 0 and [6, 4] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Display some samples from the DataLoader\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, bboxes, classes) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(od_dataloader):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# Display only the first 2 batches\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4, 4] at entry 0 and [6, 4] at entry 1"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Function to display an image with bounding boxes and class labels\n",
    "def show_image_with_boxes(image, bboxes, classes, class_names):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image.permute(1, 2, 0))  # Convert CHW to HWC format for display\n",
    "\n",
    "    for bbox, cls in zip(bboxes, classes):\n",
    "        if bbox[0] == -1:\n",
    "            break  # Skip padding bboxes\n",
    "\n",
    "        # Unpack the bounding box\n",
    "        x, y, width, height = bbox\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add the class label\n",
    "        plt.text(x, y, class_names[int(cls)], color='yellow', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display some samples from the DataLoader\n",
    "for batch_idx, (images, bboxes, classes) in enumerate(od_dataloader):\n",
    "    if batch_idx >= 2:  # Display only the first 2 batches\n",
    "        break\n",
    "\n",
    "    for img_idx in range(images.size(0)):\n",
    "        img = images[img_idx]\n",
    "        bbox = bboxes[img_idx]\n",
    "        cls = classes[img_idx]\n",
    "\n",
    "        show_image_with_boxes(img, bbox, cls, name2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc539f55-7fc3-4f47-bd54-534839f129d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
    "    img_data_all = img_batch\n",
    "    gt_bboxes_all = gt_bboxes_batch\n",
    "    gt_classes_all = gt_classes_batch\n",
    "    break\n",
    "    \n",
    "img_data_all = img_data_all[:2]\n",
    "gt_bboxes_all = gt_bboxes_all[:2]\n",
    "gt_classes_all = gt_classes_all[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81591f-43bc-41e7-9719-bdf446390858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class names\n",
    "gt_class_1 = gt_classes_all[0].long()\n",
    "gt_class_1 = [idx2name[idx.item()] for idx in gt_class_1]\n",
    "\n",
    "gt_class_2 = gt_classes_all[1].long()\n",
    "gt_class_2 = [idx2name[idx.item()] for idx in gt_class_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493fb3b5-9526-4a50-9cdd-7177d548ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = (1, 2)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 8))\n",
    "\n",
    "fig, axes = display_img(img_data_all, fig, axes)\n",
    "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0], classes=gt_class_1)\n",
    "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1], classes=gt_class_2)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
