{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b250b73-e645-447d-8431-59fc98d1753d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
    "from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e0cd04-3084-4e49-85d1-1ccc049bf5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torchaudio\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import torch, random\n",
    "import librosa, os\n",
    "import IPython.display as ipd\n",
    "from dotenv import load_dotenv\n",
    "import evaluate\n",
    "from spellchecker import SpellChecker\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d69e4a5-e1eb-4022-b673-25449a6cb6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "047b3ec5-1111-4b6e-8b77-5f0072a4ce63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "TEAM_NAME = os.getenv(\"TEAM_NAME\", \"7up\")\n",
    "TEAM_TRACK = os.getenv(\"TEAM_TRACK\", \"advanced\")\n",
    "\n",
    "\n",
    "input_dir = Path(f\"/home/jupyter/{TEAM_TRACK}\")\n",
    "# input_dir = Path(f\"../../data/{TEAM_TRACK}/train\")\n",
    "results_dir = Path(f\"/home/jupyter/{TEAM_NAME}\")\n",
    "# results_dir = Path(\"results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "data = {'image': [], 'annotations': []}\n",
    "count = 0\n",
    "\n",
    "with jsonlines.open(input_dir / \"vlm.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        if count < 2:  # Check if we have processed less than 10 entries\n",
    "            data['image'].append(obj['image'])\n",
    "            data['annotations'].append(obj['annotations'])\n",
    "            count += 1\n",
    "        else:\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c56aac-7021-49e4-b8e3-8755c5fc5d78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': ['image_0.jpg', 'image_1.jpg'],\n",
       " 'annotations': [[{'caption': 'grey missile', 'bbox': [912, 164, 48, 152]},\n",
       "   {'caption': 'red, white, and blue light aircraft',\n",
       "    'bbox': [1032, 80, 24, 28]},\n",
       "   {'caption': 'green and black missile', 'bbox': [704, 508, 76, 64]},\n",
       "   {'caption': 'white and red helicopter', 'bbox': [524, 116, 112, 48]}],\n",
       "  [{'caption': 'grey camouflage fighter jet', 'bbox': [1112, 172, 64, 36]},\n",
       "   {'caption': 'grey and white fighter plane', 'bbox': [1108, 512, 144, 48]},\n",
       "   {'caption': 'white and black drone', 'bbox': [356, 452, 48, 32]},\n",
       "   {'caption': 'white and black fighter jet', 'bbox': [404, 156, 48, 36]},\n",
       "   {'caption': 'white missile', 'bbox': [544, 112, 40, 40]},\n",
       "   {'caption': 'black and white commercial aircraft',\n",
       "    'bbox': [808, 504, 68, 68]}]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5ed474-047b-475f-8362-87b264da8899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the model and processor\n",
    "# processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "# model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "\n",
    "# # Mean and standard deviation for normalization\n",
    "# OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "# OPENAI_CLIP_STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "# # Function to preprocess the image\n",
    "# def get_preprocessed_image(pixel_values):\n",
    "#     pixel_values = pixel_values.squeeze().numpy()\n",
    "#     unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n",
    "#     unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "#     unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "#     unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "#     return unnormalized_image\n",
    "\n",
    "# # Assuming 'data' is a list or array containing images\n",
    "# for image_tensor in data[]:\n",
    "#     # Convert tensor to NumPy array\n",
    "#     image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "#     # Convert to RGB\n",
    "#     image_rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Convert to PIL image\n",
    "#     image_pil = Image.fromarray(image_rgb)\n",
    "\n",
    "#     # Preprocess the image\n",
    "#     inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
    "\n",
    "#     # Forward pass\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#     # Note: boxes need to be visualized on the padded, unnormalized image\n",
    "#     # hence we'll set the target image sizes (height, width) based on that\n",
    "\n",
    "#     unnormalized_image = get_preprocessed_image(inputs.pixel_values)\n",
    "\n",
    "#     target_sizes = torch.Tensor([unnormalized_image.size[::-1]])\n",
    "#     # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n",
    "#     results = processor.post_process_object_detection(\n",
    "#         outputs=outputs, threshold=0.2, target_sizes=target_sizes\n",
    "#     )\n",
    "\n",
    "#     boxes, scores, labels = results[0][\"boxes\"], results[0][\"scores\"], results[0][\"labels\"]\n",
    "\n",
    "#     for box, score, label in zip(boxes, scores, labels):\n",
    "#         box = [round(i, 2) for i in box.tolist()]\n",
    "#         print(f\"Detected {model.config.id2label[label.item()]} with confidence {round(score.item(), 3)} at location {box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd98a68-efc9-44d0-86b0-c3ca106dacbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "# model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "\n",
    "# # url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# # image = Image.open(requests.get(url, stream=True).raw)\n",
    "# # texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n",
    "# # inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "# for image_path in data['image']:\n",
    "#     # Load an image\n",
    "#     image_path = input_dir / \"images\" / image_path  # Change this to your image path\n",
    "#     image = cv2.imread(str(image_path))\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "#     # image = torch.tensor(image)\n",
    "    \n",
    "#     # Preprocess the image\n",
    "#     inputs = processor(texts = [\"a photo of an airplane\"], images=image, return_tensors=\"pt\")\n",
    "\n",
    "# # forward pass\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# # Note: boxes need to be visualized on the padded, unnormalized image\n",
    "# # hence we'll set the target image sizes (height, width) based on that\n",
    "\n",
    "# def get_preprocessed_image(pixel_values):\n",
    "#     pixel_values = pixel_values.squeeze().numpy()\n",
    "#     unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n",
    "#     unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "#     unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "#     unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "#     return unnormalized_image\n",
    "\n",
    "# unnormalized_image = get_preprocessed_image(inputs.pixel_values)\n",
    "\n",
    "# target_sizes = torch.Tensor([unnormalized_image.size[::-1]])\n",
    "# # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n",
    "# results = processor.post_process_object_detection(\n",
    "#     outputs=outputs, threshold=0.2, target_sizes=target_sizes\n",
    "# )\n",
    "\n",
    "# i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "# text = texts[i]\n",
    "# boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "# for box, score, label in zip(boxes, scores, labels):\n",
    "#     box = [round(i, 2) for i in box.tolist()]\n",
    "#     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddac9e11-75b5-44ca-a369-14a5a7f1e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# import skimage\n",
    "# import numpy as np\n",
    "# from PIL import Image, ImageDraw\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# checkpoint = \"google/owlv2-base-patch16-ensemble\" #google/owlv2-base-patch16-ensemble\n",
    "# detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\n",
    "\n",
    "\n",
    "# for image_path in data['image']:\n",
    "#     # Load an image\n",
    "#     image_path = input_dir / \"images\" / image_path  # Change this to your image path\n",
    "#     image = cv2.imread(str(image_path))\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image_pil = Image.fromarray(image)\n",
    "\n",
    "#     predictions = detector(\n",
    "#         image_pil,\n",
    "#         candidate_labels=[\"airplane\", \"missile\", \"drone\", \"helicopter\", \"fighter jet\"],\n",
    "#         threshold = 0.011\n",
    "#     )\n",
    "#     print(f\"Predictions for {image_path}:\")\n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     ax.imshow(image_pil)\n",
    "\n",
    "#     for prediction in predictions:\n",
    "#         label = prediction['label']\n",
    "#         score = prediction['score']\n",
    "#         box = prediction['box']\n",
    "#         print(f\"Detected {label} with confidence {round(score, 3)} at location {box}\")\n",
    "\n",
    "#         # Draw bounding box\n",
    "#         rect = patches.Rectangle(\n",
    "#             (box['xmin'], box['ymin']),\n",
    "#             box['xmax'] - box['xmin'],\n",
    "#             box['ymax'] - box['ymin'],\n",
    "#             linewidth=2,\n",
    "#             edgecolor='r',\n",
    "#             facecolor='none'\n",
    "#         )\n",
    "#         ax.add_patch(rect)\n",
    "#         plt.text(\n",
    "#             box['xmin'], box['ymin'] - 10,\n",
    "#             f\"{label}: {round(score, 3)}\",\n",
    "#             color='red',\n",
    "#             fontsize=12,\n",
    "#             bbox=dict(facecolor='white', alpha=0.5)\n",
    "#         )\n",
    "    \n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b322948a-0330-4a9c-be73-a75fe8f92965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8fff8b072941fab8dc067353c0a0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `yolov10` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:951\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 951\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:653\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    654\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'yolov10'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpatches\u001b[39;00m\n\u001b[1;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx-community/yolov10s\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# DINOv2 model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-object-detection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Load an image\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m input_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m image_path  \u001b[38;5;66;03m# Change this to your image path\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:816\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    814\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 816\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    821\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:953\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `yolov10` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "checkpoint = \"onnx-community/yolov10s\"  # DINOv2 model\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\n",
    "\n",
    "for image_path in data['image']:\n",
    "    # Load an image\n",
    "    image_path = input_dir / \"images\" / image_path  # Change this to your image path\n",
    "    image = cv2.imread(str(image_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Image.fromarray(image)\n",
    "\n",
    "    predictions = detector(\n",
    "        image_pil,\n",
    "        candidate_labels=[\"airplane\", \"missile\", \"drone\", \"helicopter\", \"fighter jet\"],\n",
    "        threshold=0.16\n",
    "    )\n",
    "    print(f\"Predictions for {image_path}:\")\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image_pil)\n",
    "\n",
    "    for prediction in predictions:\n",
    "        label = prediction['label']\n",
    "        score = prediction['score']\n",
    "        box = prediction['box']\n",
    "        \n",
    "\n",
    "        if score > 0.16:  # Check if confidence score is greater than 0.16\n",
    "            print(f\"Detected {label} with confidence {round(score, 3)} at location {box}\")\n",
    "            # Draw bounding box\n",
    "            rect = patches.Rectangle(\n",
    "                (box['xmin'], box['ymin']),\n",
    "                box['xmax'] - box['xmin'],\n",
    "                box['ymax'] - box['ymin'],\n",
    "                linewidth=2,\n",
    "                edgecolor='r',\n",
    "                facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            plt.text(\n",
    "                box['xmin'], box['ymin'] - 10,\n",
    "                f\"{label}: {round(score, 3)}\",\n",
    "                color='red',\n",
    "                fontsize=12,\n",
    "                bbox=dict(facecolor='white', alpha=0.5)\n",
    "            )\n",
    "    \n",
    "    plt.show()\n",
    "# ['GroundingDinoForObjectDetection', 'Owlv2ForObjectDetection', 'OwlViTForObjectDetection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c11ca2-80ee-4844-bd09-9857970d2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw = ImageDraw.Draw(image)\n",
    "\n",
    "# for prediction in predictions:\n",
    "#     box = prediction[\"box\"]\n",
    "#     label = prediction[\"label\"]\n",
    "#     score = prediction[\"score\"]\n",
    "\n",
    "#     xmin, ymin, xmax, ymax = box.values()\n",
    "#     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "#     draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\n",
    "\n",
    "# image\n",
    "\n",
    "# from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "# model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n",
    "# processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "# import requests\n",
    "\n",
    "# # image prompts\n",
    "# url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\n",
    "# im = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# # text prompts\n",
    "# text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n",
    "# inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")\n",
    "\n",
    "# # printing the image\n",
    "# im\n",
    "\n",
    "# import torch\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "#     target_sizes = torch.tensor([im.size[::-1]])\n",
    "#     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n",
    "\n",
    "# draw = ImageDraw.Draw(im)\n",
    "\n",
    "# scores = results[\"scores\"].tolist()\n",
    "# labels = results[\"labels\"].tolist()\n",
    "# boxes = results[\"boxes\"].tolist()\n",
    "\n",
    "# for box, score, label in zip(boxes, scores, labels):\n",
    "#     xmin, ymin, xmax, ymax = box\n",
    "#     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "#     draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")\n",
    "#     print('Query:'+ text_queries[label], round(score,2))\n",
    "#     print(f\"\\t {' '.join(str(i) for i in [xmin, ymin, xmax, ymax])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8512a406-4975-480d-913d-8093a74cdbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yolov9n.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m clip_processor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize YOLOv8 model for object detection\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov9n.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load a pre-trained YOLOv8 model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m candidate_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrey camouflage fighter jet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Function to process each image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:161\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    159\u001b[0m suffix \u001b[38;5;241m=\u001b[39m Path(weights)\u001b[38;5;241m.\u001b[39msuffix\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:700\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    699\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:634\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_modules(\n\u001b[1;32m    628\u001b[0m         {\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.yolo.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m         }\n\u001b[1;32m    633\u001b[0m     ):  \u001b[38;5;66;03m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, file  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov9n.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Initialize YOLOv8 model for object detection\n",
    "yolo_model = YOLO(\"yolov9n.pt\")  # Load a pre-trained YOLOv8 model\n",
    "\n",
    "candidate_labels = [\"grey camouflage fighter jet\"]\n",
    "\n",
    "# Function to process each image\n",
    "def process_image(image_path):\n",
    "    # Load an image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Image.fromarray(image)\n",
    "\n",
    "    # Perform object detection using YOLOv8\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image_pil)\n",
    "\n",
    "    # Track the highest confidence detection\n",
    "    highest_confidence = 0\n",
    "    best_box = None\n",
    "    best_label = None\n",
    "\n",
    "    # Process YOLO results\n",
    "    for result in results:  # Process each result in the results list\n",
    "        boxes = result.boxes\n",
    "        names = result.names\n",
    "\n",
    "        for box in boxes:\n",
    "            # Extract bounding box coordinates\n",
    "            xmin, ymin, xmax, ymax = box.xyxy[0].tolist()[:4]\n",
    "            confidence = box.conf[0].item()\n",
    "            class_id = int(box.cls[0].item())\n",
    "            label = names[class_id]\n",
    "\n",
    "            # Crop the detected object\n",
    "            cropped_image = image_pil.crop((xmin, ymin, xmax, ymax))\n",
    "\n",
    "            # Prepare the cropped image and candidate labels for CLIP model\n",
    "            inputs = clip_processor(text=candidate_labels, images=cropped_image, return_tensors=\"pt\", padding=True)\n",
    "            outputs = clip_model(**inputs)\n",
    "\n",
    "            # Get the similarity scores\n",
    "            logits_per_image = outputs.logits_per_image.softmax(dim=1)\n",
    "            probs = logits_per_image.cpu().detach().numpy()\n",
    "\n",
    "            # Find the best matching label\n",
    "            best_score = probs[0, 0]\n",
    "\n",
    "            if best_score > highest_confidence:\n",
    "                highest_confidence = best_score\n",
    "                best_box = (xmin, ymin, xmax, ymax)\n",
    "                best_label = candidate_labels[0]\n",
    "\n",
    "    if best_box and highest_confidence > 0.01:\n",
    "        xmin, ymin, xmax, ymax = best_box\n",
    "        print(f\"Detected {best_label} with confidence {round(highest_confidence, 3)} at location {(xmin, ymin, xmax, ymax)}\")\n",
    "\n",
    "        # Draw bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin),\n",
    "            xmax - xmin,\n",
    "            ymax - ymin,\n",
    "            linewidth=2,\n",
    "            edgecolor='r',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(\n",
    "            xmin, ymin - 10,\n",
    "            f\"{best_label}: {round(highest_confidence, 3)}\",\n",
    "            color='red',\n",
    "            fontsize=12,\n",
    "            bbox=dict(facecolor='white', alpha=0.5)\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for image_path in data['image']:\n",
    "    process_image(input_dir / \"images\" / image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e33ef828-f4d9-437a-ae16-cec19b9f455c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([14., 33., 33., 33.], device='cuda:0')\n",
      "conf: tensor([0.3610, 0.3252, 0.2793, 0.2616], device='cuda:0')\n",
      "data: tensor([[7.0570e+02, 5.0367e+02, 7.8017e+02, 5.6904e+02, 3.6103e-01, 1.4000e+01],\n",
      "        [9.1406e+02, 1.6726e+02, 9.6219e+02, 3.1795e+02, 3.2515e-01, 3.3000e+01],\n",
      "        [5.2833e+02, 1.1714e+02, 6.2643e+02, 1.6150e+02, 2.7928e-01, 3.3000e+01],\n",
      "        [1.0287e+03, 7.4920e+01, 1.0588e+03, 1.0951e+02, 2.6162e-01, 3.3000e+01]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (870, 1520)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[ 742.9305,  536.3507,   74.4694,   65.3706],\n",
      "        [ 938.1257,  242.6085,   48.1311,  150.6889],\n",
      "        [ 577.3758,  139.3207,   98.1010,   44.3627],\n",
      "        [1043.7175,   92.2169,   30.0991,   34.5939]], device='cuda:0')\n",
      "xywhn: tensor([[0.4888, 0.6165, 0.0490, 0.0751],\n",
      "        [0.6172, 0.2789, 0.0317, 0.1732],\n",
      "        [0.3799, 0.1601, 0.0645, 0.0510],\n",
      "        [0.6867, 0.1060, 0.0198, 0.0398]], device='cuda:0')\n",
      "xyxy: tensor([[ 705.6959,  503.6654,  780.1652,  569.0360],\n",
      "        [ 914.0602,  167.2640,  962.1913,  317.9530],\n",
      "        [ 528.3253,  117.1394,  626.4263,  161.5021],\n",
      "        [1028.6680,   74.9200, 1058.7671,  109.5139]], device='cuda:0')\n",
      "xyxyn: tensor([[0.4643, 0.5789, 0.5133, 0.6541],\n",
      "        [0.6014, 0.1923, 0.6330, 0.3655],\n",
      "        [0.3476, 0.1346, 0.4121, 0.1856],\n",
      "        [0.6768, 0.0861, 0.6966, 0.1259]], device='cuda:0')\n",
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i.boxes)\n",
    "    print(i.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9e5b0-85f6-4650-85bd-762653ea8261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import OwlV2Config, OwlV2ForObjectDetection\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.annotations = sorted(os.listdir(annotation_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        ann_path = os.path.join(self.annotation_dir, self.annotations[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(ann_path) as f:\n",
    "            annotation = json.load(f)\n",
    "        boxes = torch.tensor(annotation['boxes'], dtype=torch.float32)\n",
    "        labels = torch.tensor([self.label_to_int(label) for label in annotation['labels']])\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def label_to_int(self, label):\n",
    "        # Implement a method to convert label names to integer indices\n",
    "        pass\n",
    "\n",
    "# Load the dataset\n",
    "transform = T.Compose([\n",
    "    T.Resize((512, 512)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "dataset = CustomDataset(image_dir=\"dataset/images\", annotation_dir=\"dataset/annotations\", transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Load the model\n",
    "config = OwlV2Config()\n",
    "model = OwlV2ForObjectDetection(config)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
