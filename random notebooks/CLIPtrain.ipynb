{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1b4058-3945-494a-b6b8-5bc5ead9600c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and parse JSON annotations\n",
    "def load_annotations(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        annotations = [json.loads(line.strip()) for line in f]\n",
    "    return annotations\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomCLIPDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.annotations[idx]\n",
    "        image_path = os.path.join(self.image_dir, data['image'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Generate cropped images and corresponding captions\n",
    "        crops = []\n",
    "        captions = []\n",
    "        for ann in data['annotations']:\n",
    "            bbox = ann['bbox']\n",
    "            caption = ann['caption']\n",
    "            cropped_image = image.crop((bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]))\n",
    "            if self.transform:\n",
    "                cropped_image = self.transform(cropped_image)\n",
    "            crops.append(cropped_image)\n",
    "            captions.append(caption)\n",
    "        \n",
    "        return crops, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcb756a-44fc-4db6-bfcc-223ed53a4510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load annotations and create dataset\n",
    "json_path = '/home/jupyter/advanced/vlm.jsonl'\n",
    "image_dir = '/home/jupyter/advanced/images'\n",
    "annotations = load_annotations(json_path)\n",
    "# Split annotations into training and testing sets\n",
    "train_annotations, test_annotations = train_test_split(annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomCLIPDataset(image_dir, train_annotations, transform)\n",
    "test_dataset = CustomCLIPDataset(image_dir, test_annotations, transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ce7fd9-298d-4b95-9d4b-d686fad5fb73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568c4bb2-755e-4d1d-a693-abf6349c410b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finetuning function\n",
    "def finetune_clip(model, dataloader, epochs=3, lr=1e-5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "            images, texts = batch\n",
    "            \n",
    "            images = torch.cat(images).to(model.device)\n",
    "            #images = (images - images.min()) / (images.max() - images.min())\n",
    "            # texts = sum(texts, [])\n",
    "            texts = [item for sublist in texts for item in sublist]\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "            \n",
    "            # Define contrastive loss\n",
    "            ground_truth = torch.arange(len(logits_per_image)).long().to(model.device)\n",
    "            loss = (torch.nn.functional.cross_entropy(logits_per_image, ground_truth) + torch.nn.functional.cross_entropy(logits_per_text, ground_truth)) / 2\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader)}\")\n",
    "        \n",
    "    # Save the model, processor, and tokenizer\n",
    "    model_name = \"CLIP-large-finetuned\"\n",
    "    model.save_pretrained(f\"/home/jupyter/{model_name}\")\n",
    "    processor.save_pretrained(f\"/home/jupyter/{model_name}\")\n",
    "    tokenizer.save_pretrained(f\"/home/jupyter/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d25c4e-054c-40a1-a558-a198aad81fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clip(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images, texts = batch\n",
    "            \n",
    "            # Concatenate images and ensure they are in the range [0, 1]\n",
    "            images = torch.cat(images).to(model.device)\n",
    "            #images = (images - images.min()) / (images.max() - images.min())\n",
    "            \n",
    "            # Flatten the list of captions\n",
    "            texts = [item for sublist in texts for item in sublist]\n",
    "            \n",
    "            # Process the images and texts\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "            \n",
    "            # Define contrastive loss\n",
    "            ground_truth = torch.arange(len(logits_per_image)).long().to(model.device)\n",
    "            loss = (torch.nn.functional.cross_entropy(logits_per_image, ground_truth) + torch.nn.functional.cross_entropy(logits_per_text, ground_truth)) / 2\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Evaluation Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66da727a-d2e5-43f3-87bb-2749f82ca0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/3:   0%|          | 0/4085 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training Epoch 1/3: 100%|██████████| 4085/4085 [1:09:30<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.6475635025463313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████| 4085/4085 [1:09:15<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6463951499254934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████| 4085/4085 [1:08:48<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.6463986473538739\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CLIP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Finetune the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mfinetune_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mfinetune_clip\u001b[0;34m(model, dataloader, epochs, lr)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Save the model, processor, and tokenizer\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[43mCLIP\u001b[49m\u001b[38;5;241m-\u001b[39mlarge\u001b[38;5;241m-\u001b[39mfinetuned\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jupyter/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m processor\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jupyter/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CLIP' is not defined"
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Finetune the model\n",
    "finetune_clip(model, train_dataloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e2a1dd2-8caa-4c22-b624-d80ea34ad337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1022/1022 [06:07<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 1.6436608403396233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluate_clip(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d211fc9-c4ae-47d2-a064-b223b4cd6902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
